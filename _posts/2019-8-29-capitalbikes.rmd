---
layout: post
title: Capital Biking System
subtitle: Data Analysis and Modeling
gh-repo: https://github.com/parkervg/OpioidTwitter
tags: [Data Analyis, Modeling, Regression Analysis]
comments: true

---

rmarkdown::render_site()



```{r, message= FALSE, warning=FALSE}
library(readr)
library(alr4)
library(dplyr)
day <- read_csv("~/126 Regression/Bike-Sharing-Dataset/day.csv")


```
#Preliminary observations
Welcome to the capital bike share data set. Today we will be exploring the relationship between different variables and how they impact total bike count in future settings..  
Before we dive into exploratory data analysis, we will vet the data for any irregularites and remove them as fit. 

```{r pressure, echo=FALSE}
test0 <- lm( cnt ~ . - cnt,   data = day )
summary(test0)

```
This tells us we have other responses we must remove. The other responses are: casual and registered. 
In addtion to removing those, we will get rid of non-variables. 
Our motivation is to predict count for future years, thus we must remove the categorical variable, year. 
```{r }
day <- select(day,  c(- casual, - registered, - instant, - dteday, -yr) ) 
y <- day$cnt
test0 <- lm( y ~ . - cnt ,   data = day )
summary(test0)
```
From the summary everything appears to be fine, but futher we are going to check for redundant variables using VIF. 
If there are two varaibles with a high correlation, then one must be removed, preferably the one with the higher number.
```{r } 
vif(test0)
```
Since atemp has the highest co-variances, we will remove it. 


```{r }
day <- select(day,  c(- atemp, - cnt ))
test0 <- lm( y ~ . , data = day )
vif(test0)

```
Now that the data has been cleared of any non-variables, other potiental responses, and redundacies, we can continue to exploratory data analysis.  
 

##Exploratory Data Analysis 
```{r}
avPlots(test0)

```

Here we view that the most impactful vairbale is tempature. The other variables do have correlations, but not as much as temp.

```{r}
scatterplotMatrix(~ y + temp + windspeed + hum + factor(season) + factor(mnth) + factor(holiday) + factor(weekday) + factor(workingday) + factor(weathersit) , data =day)

```

This is ugly but as we see there must be transformations in order to attain a correlation between predictors and the response. 

###Diagnostics 
```{r}
test1 <- test0
summary(test1)
par(mfrow = c( 1, 3))
for( i in 1:3){
  plot( test1 , which = i)}
par(mfrow = c( 1, 1))

```
The residuals vs fitted and scale-location plot needs work. To this we will use MLR transformation.
First checking if we need to add a constant to any numeric variable(need non-zero numbers for transformation).

```{r}
min(day$temp)
min(day$windspeed)
min(day$hum)
```
Since humidity has value of zero we must add a constant before we proceed. 

```{r}
day$hum1 <- with(day, (hum + .001 )) 
diag1  <-  powerTransform(cbind(hum1, windspeed , temp) ~ 1, day)
summary(diag1)
```

Adding a small constant that will have a small impact on the data we find that windspeed, should have a square root transformation. 

```{r}
testTransform(diag1, lambda = c(1, .5, 1))


```
Here we get a small LRT, which is very good! Despite this we have a p-value of .2

Following with the proceedures from the MLS pdf we have:
```{r}
day_trsf <- with(day, data.frame( hum1 , sqrt(windspeed), temp ))
pairs(day_trsf)

windspeed.sqrt <- sqrt(day$windspeed)
day <- cbind( day,  windspeed.sqrt )
day <- select(day , -c(hum, windspeed) )

```
In the plots we can see a grouping between the predictors at hand, because of this it is closer to a linear relationship.


Now that we have transformed our predictors, our data could follow linearity assumptions. 

```{r}
test2 <- lm(y ~. , data = day)
summary(test2)

par(mfrow = c( 1, 3))
for( i in 1:3){
  plot( test2 , which = i)
  
} 
par(mfrow = c( 1, 1))
```
There area still issues with all 3 of our plots, despite this since our data set is of length $n = 731$ the qq-plots do not have to be perfect. 

In order to fix the violations in linearity-assumptions, we will consider a response transformation: 
```{r}
bc <- boxCox(test2) 
lambda.opt <- bc$x[which.max(bc$y)] 
lambda.opt 
```
Now the lambda is closest to .5, so we should transform our response with a square root.

```{r}
y.sqrt <- sqrt(y)
test2 <- lm(y.sqrt ~ . , data = day) 

par(mfrow = c( 1, 3))
for( i in 1:3){
  plot( test2 , which = i)
  
} 
par(mfrow = c( 1, 1))

```
Here we see that our plots are close to following linear assumptions.
With nothing else to do  we can continue to the AIC/BIC methods. 


###AIC/BIC Model selection. 
Doing the AIC/ BIC Methods, we have:
```{r}
#Inital model
m0 = lm(y.sqrt ~ temp , day)

#Full model
f = ~ season  + mnth + holiday + weekday + workingday + weathersit + temp + hum1 + windspeed.sqrt
#AIC method
m.forward = step(m0 , f ,direction = 'forward' )
```
This gives us 8 variables to use. 

Now with the BIC method 
```{r}
n=731
step(m0, f, direction = 'forward', k = log(n), trace = 0) 

```
The BIC method leaves us with 5 variables .
In addition to this I calculated goodness of fit by hand and it was 16.6.


Copying and pasting the results from both: 
```{r}
BIC <- lm(formula = y.sqrt ~ temp + weathersit + season + windspeed.sqrt + hum1, data = day)
AIC <- lm(y.sqrt ~ temp + weathersit + season + windspeed.sqrt + hum1 + holiday + weekday + workingday , data = day)
anova(BIC, AIC)
```
From the anova table, we accept the BIC model. 

Must do BIC MLR diagnostics, then make the model plots, if still not good, make a non-parallel model. 

In this chunk I will be usign the linear model BIC and factoring all categorical variables. 
```{r}
BIC.factor <- lm(formula = y.sqrt ~ temp + factor(weathersit) + factor(season) + windspeed.sqrt + hum1 , data = day)
summary(BIC.factor)
```

From the summary we see that our $R^2$ increased and Residual standard error decreased. This is good. In the chart we have an issue with the p-value for factor(weekday)[1]. This is not grounds to remove it, it just implies that the slope for factor(weekday)[1] is possibly 0. 


```{r}
par(mfrow = c( 1, 3))
for( i in 1:3){
  plot( BIC.factor , which = i)
  
} 
par(mfrow = c( 1,1))


```

Running a second diagnostic to see if we can transform and make residuals linear.
```{r}
diag2  <-  powerTransform(cbind(hum1, windspeed.sqrt , temp) ~ 1, day)
summary(diag2)

```
No futher transfermations are possible. 

Thus we must consider a non-parallel model. After a lot of testing 
```{r}
test3 <- lm(y.sqrt ~ temp* factor(weathersit)*factor(season) + hum1 + windspeed.sqrt, data = day  )
summary(test3)

par(mfrow = c(2,2))   
for( i in 1:4){
  plot(test3 , which = i )
}

par(mfrow = c(1,1))

```

In addition, we will verify it is the best 

```{r}
anova(BIC.factor, test3 )
``` 

Since we have such a small p-value for model2, we reject that the slope of non-parallel interactions are 0,
arriving at our final model. 
